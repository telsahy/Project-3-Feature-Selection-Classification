{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, f_regression\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and sampling the UCI data / basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST BATCH OF 2200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle('../data/first_batch_X.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_pickle('../data/first_batch_labels.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_000</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_990</th>\n",
       "      <th>feat_991</th>\n",
       "      <th>feat_992</th>\n",
       "      <th>feat_993</th>\n",
       "      <th>feat_994</th>\n",
       "      <th>feat_995</th>\n",
       "      <th>feat_996</th>\n",
       "      <th>feat_997</th>\n",
       "      <th>feat_998</th>\n",
       "      <th>feat_999</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68411</th>\n",
       "      <td>0.020855</td>\n",
       "      <td>0.578265</td>\n",
       "      <td>-0.136120</td>\n",
       "      <td>-0.410538</td>\n",
       "      <td>-1.772283</td>\n",
       "      <td>1.187936</td>\n",
       "      <td>0.402231</td>\n",
       "      <td>1.176466</td>\n",
       "      <td>-0.792536</td>\n",
       "      <td>-1.821266</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.492082</td>\n",
       "      <td>-0.302283</td>\n",
       "      <td>-0.259279</td>\n",
       "      <td>-1.326535</td>\n",
       "      <td>-1.993000</td>\n",
       "      <td>0.540589</td>\n",
       "      <td>-1.205038</td>\n",
       "      <td>-0.581775</td>\n",
       "      <td>-0.613431</td>\n",
       "      <td>0.365626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118486</th>\n",
       "      <td>-0.668980</td>\n",
       "      <td>0.411098</td>\n",
       "      <td>-0.818625</td>\n",
       "      <td>-0.680110</td>\n",
       "      <td>1.437646</td>\n",
       "      <td>0.638755</td>\n",
       "      <td>0.201362</td>\n",
       "      <td>0.467585</td>\n",
       "      <td>-0.347586</td>\n",
       "      <td>0.962761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337309</td>\n",
       "      <td>0.404857</td>\n",
       "      <td>0.594332</td>\n",
       "      <td>0.801810</td>\n",
       "      <td>0.108186</td>\n",
       "      <td>0.218923</td>\n",
       "      <td>-0.433100</td>\n",
       "      <td>-1.383996</td>\n",
       "      <td>1.760135</td>\n",
       "      <td>0.256725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26213</th>\n",
       "      <td>-0.255765</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>1.292950</td>\n",
       "      <td>-0.478335</td>\n",
       "      <td>-0.688653</td>\n",
       "      <td>-0.094722</td>\n",
       "      <td>0.387218</td>\n",
       "      <td>-0.938971</td>\n",
       "      <td>-1.000622</td>\n",
       "      <td>1.521842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475559</td>\n",
       "      <td>0.576874</td>\n",
       "      <td>2.402998</td>\n",
       "      <td>0.330567</td>\n",
       "      <td>1.089679</td>\n",
       "      <td>1.599995</td>\n",
       "      <td>0.173667</td>\n",
       "      <td>-0.705471</td>\n",
       "      <td>0.473086</td>\n",
       "      <td>-0.595255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121169</th>\n",
       "      <td>-0.118851</td>\n",
       "      <td>-1.572511</td>\n",
       "      <td>0.104159</td>\n",
       "      <td>1.283724</td>\n",
       "      <td>0.024256</td>\n",
       "      <td>-0.257493</td>\n",
       "      <td>0.374294</td>\n",
       "      <td>-0.893251</td>\n",
       "      <td>-0.098985</td>\n",
       "      <td>0.223367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102171</td>\n",
       "      <td>-0.256557</td>\n",
       "      <td>0.064411</td>\n",
       "      <td>-1.307146</td>\n",
       "      <td>-1.028803</td>\n",
       "      <td>-0.813137</td>\n",
       "      <td>1.492722</td>\n",
       "      <td>1.395631</td>\n",
       "      <td>1.233597</td>\n",
       "      <td>0.769766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87524</th>\n",
       "      <td>-0.269251</td>\n",
       "      <td>1.187003</td>\n",
       "      <td>-1.003767</td>\n",
       "      <td>1.106124</td>\n",
       "      <td>1.524958</td>\n",
       "      <td>0.182640</td>\n",
       "      <td>0.801729</td>\n",
       "      <td>-1.491610</td>\n",
       "      <td>-0.143916</td>\n",
       "      <td>0.399354</td>\n",
       "      <td>...</td>\n",
       "      <td>2.051286</td>\n",
       "      <td>-0.200767</td>\n",
       "      <td>-1.292400</td>\n",
       "      <td>-0.173536</td>\n",
       "      <td>0.937770</td>\n",
       "      <td>2.080136</td>\n",
       "      <td>-0.412230</td>\n",
       "      <td>2.234453</td>\n",
       "      <td>-0.906541</td>\n",
       "      <td>1.336438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat_000  feat_001  feat_002  feat_003  feat_004  feat_005  feat_006  \\\n",
       "_id                                                                            \n",
       "68411   0.020855  0.578265 -0.136120 -0.410538 -1.772283  1.187936  0.402231   \n",
       "118486 -0.668980  0.411098 -0.818625 -0.680110  1.437646  0.638755  0.201362   \n",
       "26213  -0.255765  0.040380  1.292950 -0.478335 -0.688653 -0.094722  0.387218   \n",
       "121169 -0.118851 -1.572511  0.104159  1.283724  0.024256 -0.257493  0.374294   \n",
       "87524  -0.269251  1.187003 -1.003767  1.106124  1.524958  0.182640  0.801729   \n",
       "\n",
       "        feat_007  feat_008  feat_009    ...     feat_990  feat_991  feat_992  \\\n",
       "_id                                     ...                                    \n",
       "68411   1.176466 -0.792536 -1.821266    ...    -1.492082 -0.302283 -0.259279   \n",
       "118486  0.467585 -0.347586  0.962761    ...     0.337309  0.404857  0.594332   \n",
       "26213  -0.938971 -1.000622  1.521842    ...    -0.475559  0.576874  2.402998   \n",
       "121169 -0.893251 -0.098985  0.223367    ...     0.102171 -0.256557  0.064411   \n",
       "87524  -1.491610 -0.143916  0.399354    ...     2.051286 -0.200767 -1.292400   \n",
       "\n",
       "        feat_993  feat_994  feat_995  feat_996  feat_997  feat_998  feat_999  \n",
       "_id                                                                           \n",
       "68411  -1.326535 -1.993000  0.540589 -1.205038 -0.581775 -0.613431  0.365626  \n",
       "118486  0.801810  0.108186  0.218923 -0.433100 -1.383996  1.760135  0.256725  \n",
       "26213   0.330567  1.089679  1.599995  0.173667 -0.705471  0.473086 -0.595255  \n",
       "121169 -1.307146 -1.028803 -0.813137  1.492722  1.395631  1.233597  0.769766  \n",
       "87524  -0.173536  0.937770  2.080136 -0.412230  2.234453 -0.906541  1.336438  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id\n",
       "68411     1\n",
       "118486    1\n",
       "26213     1\n",
       "121169    0\n",
       "87524     0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2200, 1000), (2200,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECTKBEST for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Y Labels instead of dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = .3,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=20, score_func=<function f_classif at 0x7f270e67f400>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb = SelectKBest(k=20)\n",
    "\n",
    "skb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  52,  78, 269, 315, 341, 507, 524, 611, 680, 681, 701, 736,\n",
       "       745, 769, 808, 829, 891, 907, 920])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb_feats = np.where(skb.get_support())[0]\n",
    "skb_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT FROM MODEL for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Y Labels instead of dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm = SelectFromModel(LogisticRegression(), threshold='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        prefit=False, threshold='mean')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,  11,  13,  18,  19,  22,  23,  27,  29,  31,  33,  34,\n",
       "        36,  37,  41,  48,  49,  51,  53,  57,  61,  62,  63,  65,  67,\n",
       "        73,  77,  80,  81,  85,  90,  93,  96,  98,  99, 106, 107, 108,\n",
       "       111, 112, 114, 115, 118, 124, 126, 128, 129, 133, 135, 146, 148,\n",
       "       149, 150, 151, 155, 156, 158, 159, 162, 163, 168, 180, 182, 185,\n",
       "       189, 190, 191, 193, 194, 196, 199, 202, 203, 205, 207, 211, 212,\n",
       "       213, 214, 216, 217, 223, 225, 229, 234, 235, 241, 242, 245, 246,\n",
       "       251, 254, 257, 261, 262, 263, 269, 273, 274, 276, 277, 278, 281,\n",
       "       282, 283, 285, 294, 296, 300, 301, 305, 306, 308, 310, 314, 323,\n",
       "       330, 331, 334, 335, 344, 345, 346, 347, 348, 353, 354, 358, 359,\n",
       "       360, 363, 366, 368, 375, 376, 378, 380, 382, 386, 387, 388, 392,\n",
       "       393, 394, 400, 404, 408, 411, 412, 413, 414, 416, 421, 422, 424,\n",
       "       425, 427, 428, 430, 431, 433, 435, 436, 438, 439, 441, 445, 446,\n",
       "       448, 450, 451, 452, 453, 457, 458, 462, 464, 465, 466, 468, 469,\n",
       "       470, 473, 474, 477, 478, 480, 482, 483, 484, 485, 486, 488, 493,\n",
       "       494, 496, 499, 500, 504, 506, 507, 508, 510, 514, 516, 517, 519,\n",
       "       521, 524, 525, 527, 529, 534, 536, 538, 540, 541, 554, 555, 558,\n",
       "       559, 561, 562, 563, 566, 569, 571, 577, 579, 581, 586, 593, 594,\n",
       "       599, 603, 605, 606, 614, 615, 617, 618, 620, 621, 622, 624, 625,\n",
       "       626, 630, 631, 634, 651, 652, 653, 655, 656, 658, 665, 666, 670,\n",
       "       671, 672, 673, 675, 677, 679, 680, 681, 684, 685, 687, 688, 695,\n",
       "       697, 701, 702, 703, 706, 707, 708, 711, 712, 714, 717, 719, 726,\n",
       "       727, 733, 737, 738, 741, 744, 745, 746, 751, 754, 758, 766, 767,\n",
       "       768, 769, 771, 773, 774, 776, 777, 778, 779, 780, 781, 782, 785,\n",
       "       787, 790, 793, 794, 795, 799, 803, 804, 805, 806, 808, 815, 818,\n",
       "       819, 820, 822, 824, 826, 829, 832, 833, 834, 842, 844, 845, 848,\n",
       "       849, 851, 854, 856, 857, 866, 872, 873, 876, 877, 878, 879, 880,\n",
       "       881, 883, 885, 889, 891, 894, 897, 898, 899, 900, 903, 907, 909,\n",
       "       915, 918, 920, 921, 922, 926, 932, 934, 935, 942, 943, 944, 945,\n",
       "       946, 947, 948, 949, 951, 957, 959, 960, 961, 962, 963, 967, 970,\n",
       "       971, 972, 973, 974, 977, 978, 980, 985, 989, 991, 992, 993, 994,\n",
       "       995, 996, 997, 999])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm_feats = np.where(sfm.get_support())[0]\n",
    "sfm_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEARSON CORRELATION MASKING For Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = list(corr_df[corr_df[corr_df.abs() >.5].count() > 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feat_257',\n",
       " 'feat_269',\n",
       " 'feat_308',\n",
       " 'feat_315',\n",
       " 'feat_336',\n",
       " 'feat_341',\n",
       " 'feat_395',\n",
       " 'feat_504',\n",
       " 'feat_526',\n",
       " 'feat_639',\n",
       " 'feat_681',\n",
       " 'feat_701',\n",
       " 'feat_724',\n",
       " 'feat_736',\n",
       " 'feat_769',\n",
       " 'feat_808',\n",
       " 'feat_829',\n",
       " 'feat_867',\n",
       " 'feat_920',\n",
       " 'feat_956']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEARSON CORRELATION MASKING technique returns the following features that corroborate with SelectKBest:\n",
    "\n",
    "341, 681, 701, 736, 769, 808, 829, 920\n",
    "\n",
    "### PEARSON CORRELATION MASKING technique returns the following features that corroborate with SelectFromModel:\n",
    "\n",
    "257, 269, 308, 504, 681, 701, 769, 808, 829, 920, 956"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, SelectKBest found 8 informative/redundant features that were produced by the pearson correlation technique, while SelectFromModel confirmed 11. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND BATCH OF 2200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = pd.read_pickle('../data/second_batch_X.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2 = pd.read_pickle('../data/second_batch_labels.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECTKBEST for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Y Labels instead of dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2,\n",
    "                                                    y_2,\n",
    "                                                    test_size = .3,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=20, score_func=<function f_classif at 0x7f270e67f400>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb = SelectKBest(k=20)\n",
    "\n",
    "skb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54, 199, 269, 315, 341, 504, 526, 546, 639, 681, 701, 720, 736,\n",
       "       769, 805, 808, 829, 867, 907, 920])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb_feats = np.where(skb.get_support())[0]\n",
    "skb_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT FROM MODEL for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Y Labels instead of dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm = SelectFromModel(LogisticRegression(), threshold='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        prefit=False, threshold='mean')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   3,   4,   5,   6,   8,  11,  12,  13,  20,  25,  26,  29,\n",
       "        30,  32,  37,  39,  41,  45,  46,  48,  53,  54,  55,  58,  59,\n",
       "        62,  63,  65,  68,  69,  71,  72,  73,  78,  80,  81,  82,  85,\n",
       "        87,  89,  92,  94,  98,  99, 100, 103, 105, 106, 112, 113, 116,\n",
       "       117, 119, 120, 124, 125, 129, 130, 132, 135, 138, 139, 143, 144,\n",
       "       146, 147, 151, 153, 158, 160, 162, 164, 166, 167, 168, 171, 175,\n",
       "       176, 180, 181, 186, 187, 188, 189, 190, 192, 197, 199, 201, 202,\n",
       "       203, 205, 206, 207, 209, 213, 215, 217, 222, 229, 232, 234, 235,\n",
       "       240, 242, 243, 245, 248, 249, 255, 257, 263, 266, 269, 270, 271,\n",
       "       272, 277, 278, 280, 284, 285, 287, 289, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 305, 306, 310, 317, 318, 319, 322, 323, 326,\n",
       "       330, 331, 333, 340, 342, 343, 345, 350, 354, 355, 364, 367, 371,\n",
       "       373, 375, 377, 378, 381, 382, 385, 391, 392, 397, 399, 405, 411,\n",
       "       415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 427, 433,\n",
       "       435, 440, 442, 443, 446, 447, 448, 453, 455, 457, 459, 463, 464,\n",
       "       466, 470, 472, 473, 475, 481, 482, 486, 487, 492, 494, 495, 498,\n",
       "       499, 502, 504, 509, 511, 514, 515, 516, 517, 518, 520, 524, 525,\n",
       "       526, 527, 530, 537, 539, 541, 542, 544, 546, 547, 548, 550, 552,\n",
       "       556, 557, 562, 563, 571, 572, 574, 575, 576, 579, 584, 587, 588,\n",
       "       592, 595, 600, 605, 606, 607, 608, 612, 613, 614, 615, 618, 623,\n",
       "       625, 627, 634, 636, 637, 638, 643, 645, 646, 647, 650, 652, 653,\n",
       "       657, 658, 659, 662, 663, 670, 671, 678, 679, 681, 683, 686, 687,\n",
       "       689, 690, 701, 704, 705, 707, 708, 709, 711, 712, 715, 716, 718,\n",
       "       720, 723, 727, 730, 731, 738, 739, 740, 744, 746, 747, 749, 751,\n",
       "       753, 755, 756, 757, 759, 761, 767, 769, 770, 773, 777, 783, 784,\n",
       "       785, 787, 789, 790, 793, 796, 798, 800, 801, 803, 805, 806, 808,\n",
       "       809, 811, 813, 814, 816, 817, 820, 822, 824, 825, 828, 829, 830,\n",
       "       831, 833, 834, 839, 843, 845, 847, 848, 849, 855, 860, 863, 866,\n",
       "       868, 871, 875, 877, 878, 884, 886, 889, 890, 891, 895, 896, 897,\n",
       "       898, 901, 902, 907, 909, 910, 913, 916, 917, 920, 928, 929, 932,\n",
       "       933, 937, 939, 941, 947, 949, 951, 953, 959, 960, 964, 966, 967,\n",
       "       969, 972, 975, 976, 977, 978, 980, 981, 985, 987, 990, 992, 993,\n",
       "       998, 999])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm_feats = np.where(sfm.get_support())[0]\n",
    "sfm_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following features are corroborated in both SelectKBest and SelectFromModel\n",
    "\n",
    "54, 199, 269, 504, 526, 546, 681, 701, 720, 769, 805, 808, 829, 907, 920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df_2 = X_2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_2 = list(corr_df_2[corr_df_2[corr_df_2.abs() >.5].count() > 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feat_257',\n",
       " 'feat_269',\n",
       " 'feat_308',\n",
       " 'feat_315',\n",
       " 'feat_336',\n",
       " 'feat_341',\n",
       " 'feat_395',\n",
       " 'feat_504',\n",
       " 'feat_526',\n",
       " 'feat_639',\n",
       " 'feat_681',\n",
       " 'feat_701',\n",
       " 'feat_724',\n",
       " 'feat_736',\n",
       " 'feat_769',\n",
       " 'feat_808',\n",
       " 'feat_829',\n",
       " 'feat_867',\n",
       " 'feat_920',\n",
       " 'feat_956']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEARSON CORRELATION MASKING technique returns the following features that corroborate with both SelectKBest and SelectFromModel:\n",
    "\n",
    "269, 504, 526, 681, 701, 769, 808, 829, 920\n",
    "\n",
    "Note: All of the above features were also confirmed in the first sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIRD BATCH OF 2200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = pd.read_pickle('../data/third_batch_X.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_3 = pd.read_pickle('../data/third_batch_labels.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECTKBEST for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Y Labels instead of dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_3,\n",
    "                                                    y_3,\n",
    "                                                    test_size = .3,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=20, score_func=<function f_classif at 0x7f270e67f400>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb = SelectKBest(k=20)\n",
    "\n",
    "skb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([137, 269, 315, 336, 341, 395, 504, 571, 649, 681, 701, 724, 736,\n",
       "       769, 792, 808, 826, 829, 838, 920])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb_feats = np.where(skb.get_support())[0]\n",
    "skb_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT FROM MODEL for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Y Labels instead of dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm = SelectFromModel(LogisticRegression(), threshold='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        prefit=False, threshold='mean')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[137, 269, 315, 336, 341, 395, 504, 571, 649, 681, 701, 724, 736,\n",
    "       769, 792, 808, 826, 829, 838, 920])\n",
    "\n",
    "137, 269, 336, 504, 571, 649, 681, 701, 736, 769, 792, 808, 826, 838, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   3,   5,   8,   9,  10,  13,  14,  15,  17,  18,  19,  21,\n",
       "        26,  29,  31,  32,  34,  38,  40,  41,  43,  44,  54,  62,  63,\n",
       "        64,  66,  75,  76,  78,  79,  84,  85,  87,  90,  91,  92,  93,\n",
       "        94,  96,  97, 103, 105, 107, 108, 109, 115, 116, 117, 119, 120,\n",
       "       124, 127, 128, 129, 130, 132, 134, 135, 136, 137, 138, 141, 143,\n",
       "       144, 148, 149, 150, 151, 153, 155, 156, 158, 162, 164, 165, 166,\n",
       "       167, 168, 169, 170, 178, 179, 180, 183, 184, 187, 188, 192, 193,\n",
       "       194, 198, 201, 202, 205, 210, 211, 214, 215, 216, 218, 222, 224,\n",
       "       231, 232, 233, 234, 235, 238, 239, 240, 241, 242, 243, 244, 245,\n",
       "       247, 251, 253, 257, 258, 260, 262, 264, 267, 268, 269, 270, 279,\n",
       "       280, 281, 284, 285, 287, 289, 290, 292, 298, 299, 300, 305, 311,\n",
       "       312, 313, 316, 320, 322, 329, 335, 336, 338, 340, 342, 343, 344,\n",
       "       346, 347, 353, 356, 358, 359, 363, 373, 374, 375, 377, 378, 380,\n",
       "       381, 385, 386, 394, 397, 399, 402, 403, 405, 406, 407, 425, 426,\n",
       "       427, 430, 431, 433, 435, 436, 437, 440, 443, 444, 445, 447, 448,\n",
       "       450, 454, 457, 459, 461, 464, 465, 473, 476, 478, 481, 485, 487,\n",
       "       489, 491, 493, 494, 495, 497, 502, 503, 504, 505, 506, 507, 509,\n",
       "       513, 514, 520, 524, 528, 530, 531, 532, 534, 537, 538, 539, 541,\n",
       "       543, 548, 551, 552, 556, 558, 559, 561, 562, 570, 571, 574, 578,\n",
       "       579, 581, 585, 587, 588, 592, 593, 595, 596, 597, 598, 599, 600,\n",
       "       602, 604, 605, 606, 608, 611, 618, 619, 621, 624, 626, 627, 628,\n",
       "       631, 642, 644, 648, 649, 651, 652, 653, 656, 657, 661, 664, 667,\n",
       "       669, 677, 681, 684, 688, 689, 690, 695, 696, 697, 701, 706, 707,\n",
       "       708, 709, 710, 713, 715, 716, 719, 721, 725, 727, 728, 730, 731,\n",
       "       732, 733, 736, 737, 739, 740, 742, 743, 744, 745, 746, 748, 749,\n",
       "       757, 763, 769, 770, 771, 772, 774, 775, 776, 778, 779, 780, 781,\n",
       "       782, 784, 787, 788, 792, 793, 795, 803, 804, 808, 809, 815, 816,\n",
       "       821, 822, 825, 826, 827, 831, 832, 834, 835, 837, 838, 839, 840,\n",
       "       841, 842, 843, 844, 849, 850, 852, 853, 855, 859, 864, 868, 869,\n",
       "       872, 874, 877, 879, 880, 881, 882, 884, 886, 887, 889, 890, 891,\n",
       "       893, 894, 897, 898, 900, 905, 906, 907, 910, 911, 923, 931, 934,\n",
       "       938, 940, 941, 943, 945, 946, 954, 958, 963, 964, 965, 968, 969,\n",
       "       970, 974, 976, 977, 979, 982, 987, 992, 994, 997, 999])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm_feats = np.where(sfm.get_support())[0]\n",
    "sfm_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following features are corroborated in both SelectKBest and SelectFromModel\n",
    "\n",
    "137, 269, 336, 504, 571, 649, 681, 701, 736, 769, 792, 808, 826, 838, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df_3 = X_3.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_3 = list(corr_df_3[corr_df_3[corr_df_3.abs() >.5].count() > 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feat_257',\n",
       " 'feat_269',\n",
       " 'feat_308',\n",
       " 'feat_315',\n",
       " 'feat_336',\n",
       " 'feat_341',\n",
       " 'feat_395',\n",
       " 'feat_504',\n",
       " 'feat_526',\n",
       " 'feat_639',\n",
       " 'feat_681',\n",
       " 'feat_701',\n",
       " 'feat_724',\n",
       " 'feat_736',\n",
       " 'feat_769',\n",
       " 'feat_808',\n",
       " 'feat_829',\n",
       " 'feat_867',\n",
       " 'feat_920',\n",
       " 'feat_956']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEARSON CORRELATION MASKING technique returns the following features that corroborate with both SelectKBest and SelectFromModel:\n",
    "\n",
    "504, 681, 701, 769, 808\n",
    "\n",
    "Note: All of the above features were also confirmed in the first and second samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
